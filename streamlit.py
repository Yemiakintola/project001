# -*- coding: utf-8 -*-
"""streamlit

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nCzlKnYcbVnEZl2JiXj4fyI2_00rHQs2
"""

import streamlit as st
import pandas as pd
import joblib
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from feature_engineering_module import create_input_engineered_features, create_input_engineered_features_df


# Load the trained model
# Make sure the 'best_student_performance_prediction_pipeline.pkl' file is in the same directory
try:
    best_forest_reg = joblib.load('best_student_performance_prediction_pipeline.pkl')
    st.success("Model loaded successfully!")
except FileNotFoundError:
    st.error("Error: Model file 'student_performance_predictor.pkl' not found. Please ensure it's in the same directory.")
    best_forest_reg = None

# --- Load the same encoder/scaler objects fitted during training ---
# You need to save and load the StandardScaler, OneHotEncoder, and LabelEncoder
# objects that you fitted on your *training* data.
# Replace these placeholders with your actual loading code.
try:
    # Example: Load a saved scaler (replace with your actual saved file)
    # scaler = joblib.load('scaler.pkl')
    # ohe = joblib.load('ohe.pkl') # Load your OneHotEncoder
    # le = joblib.load('le.pkl') # Load your LabelEncoder

    # For demonstration, I'll create dummy ones. Replace with your actual fitted objects.
    # You will need to fit these on a sample of your training data if you haven't saved them.
    scaler = StandardScaler()
    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
    le = LabelEncoder()

    # Fit dummy encoders/scaler on some sample data structure similar to your training data
    # In a real app, you would load the fitted objects, not fit them here.
    sample_data = pd.DataFrame({
        'Age': [18, 19, 20],
        'StudyTimeWeekly': [5, 10, 15],
        'Absences': [1, 5, 10],
        'Gender': ['Male', 'Female', 'Male'],
        'Ethnicity': ['Caucasian', 'Asian', 'African American'],
        'ParentalEducation': ["Bachelor's degree", "High School Diploma", "Some College"],
        'Tutoring': [0, 1, 0],
        'ParentalSupport': [1, 2, 3],
        'Extracurricular': [1, 0, 1],
        'Sports': [0, 1, 0],
        'Music': [1, 0, 1],
        'Volunteering': [0, 0, 1],
        'GradeClass': [1, 2, 3]
    })
    # Fit the encoders and scaler on this sample structure to get the column names
    # In a real scenario, you would load the already fitted objects.
    categorical_cols = ['Gender', 'Ethnicity', 'ParentalEducation', 'Tutoring', 'ParentalSupport', 'Extracurricular', 'Sports', 'Music', 'Volunteering', 'GradeClass']
    numerical_cols = ['Age', 'StudyTimeWeekly', 'Absences'] # Exclude GPA here as it's the target

    sample_encoded = sample_data.copy()
    for col in categorical_cols:
        if len(sample_data[col].unique()) > 2:
            encoded_data = ohe.fit_transform(sample_encoded[[col]])
            encoded_df = pd.DataFrame(encoded_data, columns=ohe.get_feature_names_out([col]))
            sample_encoded = pd.concat([sample_encoded, encoded_df], axis=1).drop(col, axis=1)
        else:
            sample_encoded[col] = le.fit_transform(sample_encoded[col])
    scaler.fit(sample_encoded[numerical_cols]) # Fit the scaler on numerical parts of sample

    st.success("Preprocessing objects loaded/initialized.")
except Exception as e:
    st.error(f"Error loading preprocessing objects: {e}")
    scaler = None
    ohe = None
    le = None


# --- Streamlit App Title and Description ---
st.title("Student Performance Predictor")
st.write("Predict student GPA based on various factors.")

if best_forest_reg and scaler and ohe and le:

    # --- Input Fields for Features ---
    st.header("Enter Student Information:")

    age = st.slider("Age", 10, 25, 16)
    study_time = st.slider("Weekly Study Time (hours)", 0, 30, 10)
    absences = st.slider("Number of Absences", 0, 50, 5)

    # Assuming you have these categories from your original data
    gender = st.selectbox("Gender", ['Male', 'Female'])
    ethnicity = st.selectbox("Ethnicity", ['Caucasian', 'Asian', 'African American', 'Hispanic', 'Other'])
    parental_education = st.selectbox("Parental Education", ["High School Diploma", "Some College", "Bachelor's degree", "Master's Degree", "PhD"])
    tutoring = st.selectbox("Tutoring", [0, 1], format_func=lambda x: 'No' if x == 0 else 'Yes') # Assuming 0/1 encoding
    parental_support = st.selectbox("Parental Support", [1, 2, 3], format_func=lambda x: f'Level {x}') # Assuming 1/2/3 levels
    extracurricular = st.selectbox("Extracurricular Activities", [0, 1], format_func=lambda x: 'No' if x == 0 else 'Yes')
    sports = st.selectbox("Sports", [0, 1], format_func=lambda x: 'No' if x == 0 else 'Yes')
    music = st.selectbox("Music", [0, 1], format_func=lambda x: 'No' if x == 0 else 'Yes')
    volunteering = st.selectbox("Volunteering", [0, 1], format_func=lambda x: 'No' if x == 0 else 'Yes')
    grade_class = st.selectbox("Grade Class", [1, 2, 3, 4, 5]) # Assuming grade classes are 1-5 after cleaning

    # --- Make Prediction Button ---
    if st.button("Predict GPA"):

        # --- Prepare Data for Prediction ---
        input_data = pd.DataFrame({
            'Age': [age],
            'StudyTimeWeekly': [study_time],
            'Absences': [absences],
            'Gender': [gender],
            'Ethnicity': [ethnicity],
            'ParentalEducation': [parental_education],
            'Tutoring': [tutoring],
            'ParentalSupport': [parental_support],
            'Extracurricular': [extracurricular],
            'Sports': [sports],
            'Music': [music],
            'Volunteering': [volunteering],
            'GradeClass': [grade_class]
        })

        # --- Apply the SAME Preprocessing as Training ---

        # 1. Handle the 'GradeClass' inconsistency if present in new data (replace 0 with mode)
        # You might need to load the mode calculated during training
        # For demonstration, using a dummy mode
        mode_grade_class = 2 # Replace with your actual calculated mode
        input_data['GradeClass'] = input_data['GradeClass'].replace(0, mode_grade_class)


        # 2. Encode categorical features
        input_encoded = input_data.copy()
        for col in categorical_cols:
            if len(input_data[col].unique()) > 2:
                # Use the fitted OneHotEncoder
                encoded_data = ohe.transform(input_encoded[[col]])
                encoded_df = pd.DataFrame(encoded_data, columns=ohe.get_feature_names_out([col]))
                input_encoded = pd.concat([input_encoded, encoded_df], axis=1).drop(col, axis=1)
            else:
                # Use the fitted LabelEncoder
                # Ensure the input value is in the known classes
                try:
                    input_encoded[col] = le.transform(input_encoded[col])
                except ValueError as e:
                    st.error(f"Error encoding {col}: {e}. Please ensure the input value is valid.")
                    st.stop() # Stop execution if encoding fails


        # 3. Scale numerical features
        # Use the fitted StandardScaler
        input_scaled = input_encoded.copy()
        input_scaled[numerical_cols] = scaler.transform(input_scaled[numerical_cols])


        # 4. Create interaction and polynomial features
        # Ensure you have the necessary columns after encoding and scaling
        # Check if the required columns exist before creating interaction features
        required_interaction_cols = ['GPA', 'StudyTimeWeekly', 'Absences'] # GPA is not in input, so this interaction needs adjustment

        # Re-think interaction features for prediction:
        # Since GPA is the target, you cannot use it to create interaction features for prediction.
        # You should only use features available *before* predicting GPA.
        # Let's adjust the feature engineering for prediction input.
        # Assuming your original interaction features like 'GPA_StudyTime_Interaction'
        # were calculated using the target variable, you can't use them directly here.
        # If you engineered interaction features between independent variables, you can include them.
        # Based on your notebook, you created 'GPA_StudyTime_Interaction' etc., which
        # likely means these weren't used as features for the model predicting GPA.
        # *Self-correction*: Rereading your notebook, 'GPA' is included in `numerical_cols`
        # which are scaled *before* interaction features are created, and then these
        # interaction features are used as input `X`. This implies the model is
        # trained on data where the target (GPA) is used in creating features, which
        # is a form of data leakage and would lead to overly optimistic performance
        # during training and evaluation, but wouldn't work for predicting on new data.
        #
        # *Revised Approach for Streamlit*: We cannot use 'GPA' to create interaction features
        # for making a new prediction. We should only use the original independent variables
        # and their interactions/polynomials *if* those interactions/polynomials were created
        # using only the independent variables.
        #
        # Let's assume the interaction features you intended to use for prediction were
        # interactions between independent variables, or polynomials of independent variables.
        # I'll recreate based on independent variables available to the app.

        # Create interaction features (using independent variables)
        input_scaled['StudyTimeAbsences_Interaction'] = input_scaled['StudyTimeWeekly'] * input_scaled['Absences']
        # Add other interaction features you might have intended that don't use GPA

        # Create polynomial features (using independent variables)
        input_scaled['StudyTimeWeekly_Squared'] = input_scaled['StudyTimeWeekly'] ** 2
        input_scaled['Absences_Squared'] = input_scaled['Absences'] ** 2
        # Add other polynomial features you might have intended that don't use GPA


        # 5. Align columns with training data
        # This is crucial. The order and presence of columns must match X_train
        # You need to get the column names from your X_train after all preprocessing
        # and ensure your input_scaled DataFrame has the same columns in the same order.
        # Missing columns should be added with a value of 0.

        # Get training columns (assuming you saved or can access them)
        # Example: train_cols = joblib.load('train_cols.pkl') # Save your X_train.columns

        # For demonstration, I'll try to infer columns based on dummy fitting.
        # In a real app, LOAD your saved X_train.columns!
        try:
             train_cols = scaler.feature_names_in_ # This might not capture OHE columns well
             # A better way is to save the columns after creating X_train:
             # joblib.dump(X_train.columns, 'train_cols.pkl')
             # train_cols = joblib.load('train_cols.pkl')
             st.success("Training columns retrieved.")
        except Exception as e:
             st.error(f"Error retrieving training columns: {e}")
             st.stop()

        # Add missing columns to the input data with default value 0
        for col in train_cols:
            if col not in input_scaled.columns:
                input_scaled[col] = 0

        # Ensure the order of columns matches the training data
        input_prepared = input_scaled[train_cols]


        # --- Make Prediction ---
        prediction = best_forest_reg.predict(input_prepared)

        # --- Display Prediction ---
        st.subheader("Predicted GPA:")
        st.write(f"{prediction[0]:.2f}") # Display the predicted GPA, formatted to 2 decimal places

else:
    st.warning("Model or preprocessing objects not loaded. Cannot make predictions.")

st.markdown("---")
st.write("This app predicts student GPA based on the Random Forest model trained on the provided dataset.")
